{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2515305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import ase.io\n",
    "import hydra\n",
    "import torch\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from pymatgen.core.structure import Structure\n",
    "from pymatgen.io.ase import AseAtomsAdaptor\n",
    "from tqdm import tqdm\n",
    "\n",
    "from mattergen.common.data.chemgraph import ChemGraph\n",
    "from mattergen.common.data.collate import collate\n",
    "from mattergen.common.data.condition_factory import ConditionLoader\n",
    "from mattergen.common.data.num_atoms_distribution import NUM_ATOMS_DISTRIBUTIONS\n",
    "from mattergen.common.data.types import TargetProperty\n",
    "from mattergen.common.utils.data_utils import lattice_matrix_to_params_torch\n",
    "from mattergen.common.utils.eval_utils import (\n",
    "    MatterGenCheckpointInfo,\n",
    "    get_crystals_list,\n",
    "    load_model_diffusion,\n",
    "    make_structure,\n",
    "    save_structures,\n",
    ")\n",
    "from mattergen.common.utils.globals import DEFAULT_SAMPLING_CONFIG_PATH, get_device\n",
    "from mattergen.diffusion.lightning_module import DiffusionLightningModule\n",
    "from mattergen.diffusion.sampling.pc_sampler import PredictorCorrector\n",
    "from mattergen.generator import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48b19b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_samples_from_sampler(\n",
    "    sampler: PredictorCorrector,\n",
    "    condition_loader: ConditionLoader,\n",
    "    properties_to_condition_on: TargetProperty | None = None,\n",
    "    output_path: Path | None = None,\n",
    "    cfg: DictConfig | None = None,\n",
    "    record_trajectories: bool = True,\n",
    ") -> list:\n",
    "    properties_to_condition_on = properties_to_condition_on or {}\n",
    "    assert all([key in sampler.diffusion_module.model.cond_fields_model_was_trained_on for key in properties_to_condition_on.keys()])  # type: ignore\n",
    "\n",
    "    all_samples_list = []\n",
    "    all_trajs_list = []\n",
    "    for conditioning_data, mask in tqdm(condition_loader, desc=\"Generating samples\"):\n",
    "        if record_trajectories:\n",
    "            sample, mean, intermediate_samples = sampler.sample_with_record(conditioning_data, mask)\n",
    "            all_trajs_list.extend(list_of_time_steps_to_list_of_trajectories(intermediate_samples))\n",
    "        else:\n",
    "            sample, mean = sampler.sample(conditioning_data, mask)\n",
    "        all_samples_list.extend(mean.to_data_list())\n",
    "    all_samples = collate(all_samples_list)\n",
    "    assert isinstance(all_samples, ChemGraph)\n",
    "    lengths, angles = lattice_matrix_to_params_torch(all_samples.cell)\n",
    "    all_samples = all_samples.replace(lengths=lengths, angles=angles)\n",
    "\n",
    "    generated_strucs = structure_from_model_output(\n",
    "        all_samples[\"pos\"].reshape(-1, 3),\n",
    "        all_samples[\"atomic_numbers\"].reshape(-1),\n",
    "        all_samples[\"lengths\"].reshape(-1, 3),\n",
    "        all_samples[\"angles\"].reshape(-1, 3),\n",
    "        all_samples[\"num_atoms\"].reshape(-1),\n",
    "    )\n",
    "\n",
    "    if output_path is not None:\n",
    "        assert cfg is not None\n",
    "        save_structures(output_path, generated_strucs)\n",
    "\n",
    "        if record_trajectories:\n",
    "            dump_trajectories(\n",
    "                output_path=output_path,\n",
    "                all_trajs_list=all_trajs_list,\n",
    "            )\n",
    "\n",
    "    return generated_strucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7300579e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab24bb381ca849d787da227b8c5d208b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "last.ckpt:   0%|          | 0.00/513M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb50614334714353ace95f884648f0d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.yaml:   0%|          | 0.00/7.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model config:\n",
      "adapter:\n",
      "  adapter:\n",
      "    _target_: mattergen.adapter.GemNetTAdapter\n",
      "    atom_type_diffusion: mask\n",
      "    denoise_atom_types: true\n",
      "    gemnet:\n",
      "      _target_: mattergen.common.gemnet.gemnet_ctrl.GemNetTCtrl\n",
      "      atom_embedding:\n",
      "        _target_: mattergen.common.gemnet.layers.embedding_block.AtomEmbedding\n",
      "        emb_size: 512\n",
      "        with_mask_type: true\n",
      "      condition_on_adapt:\n",
      "      - space_group\n",
      "      cutoff: 7.0\n",
      "      emb_size_atom: 512\n",
      "      emb_size_edge: 512\n",
      "      latent_dim: 512\n",
      "      max_cell_images_per_dim: 5\n",
      "      max_neighbors: 50\n",
      "      num_blocks: 4\n",
      "      num_targets: 1\n",
      "      otf_graph: true\n",
      "      regress_stress: true\n",
      "      scale_file: /scratch/amlt_code/mattergen/common/gemnet/gemnet-dT.json\n",
      "    hidden_dim: 512\n",
      "    property_embeddings: {}\n",
      "    property_embeddings_adapt:\n",
      "      space_group:\n",
      "        _target_: mattergen.property_embeddings.PropertyEmbedding\n",
      "        conditional_embedding_module:\n",
      "          _target_: mattergen.property_embeddings.SpaceGroupEmbeddingVector\n",
      "          hidden_dim: 512\n",
      "        name: space_group\n",
      "        scaler:\n",
      "          _target_: torch.nn.Identity\n",
      "        unconditional_embedding_module:\n",
      "          _target_: mattergen.property_embeddings.EmbeddingVector\n",
      "          hidden_dim: 512\n",
      "  full_finetuning: true\n",
      "  load_epoch: last\n",
      "  model_path: checkpoints/mattergen_base\n",
      "data_module:\n",
      "  _recursive_: true\n",
      "  _target_: mattergen.common.data.datamodule.CrystDataModule\n",
      "  average_density: 0.05771451654022283\n",
      "  batch_size:\n",
      "    train: 64\n",
      "    val: 64\n",
      "  dataset_transforms:\n",
      "  - _partial_: true\n",
      "    _target_: mattergen.common.data.dataset_transform.filter_sparse_properties\n",
      "  max_epochs: 2200\n",
      "  num_workers:\n",
      "    train: 0\n",
      "    val: 0\n",
      "  properties:\n",
      "  - space_group\n",
      "  root_dir: datasets/cache/alex_mp_20\n",
      "  train_dataset:\n",
      "    _target_: mattergen.common.data.dataset.CrystalDataset.from_cache_path\n",
      "    cache_path: datasets/cache/alex_mp_20/train\n",
      "    dataset_transforms:\n",
      "    - _partial_: true\n",
      "      _target_: mattergen.common.data.dataset_transform.filter_sparse_properties\n",
      "    properties:\n",
      "    - space_group\n",
      "    transforms:\n",
      "    - _partial_: true\n",
      "      _target_: mattergen.common.data.transform.symmetrize_lattice\n",
      "    - _partial_: true\n",
      "      _target_: mattergen.common.data.transform.set_chemical_system_string\n",
      "  transforms:\n",
      "  - _partial_: true\n",
      "    _target_: mattergen.common.data.transform.symmetrize_lattice\n",
      "  - _partial_: true\n",
      "    _target_: mattergen.common.data.transform.set_chemical_system_string\n",
      "  val_dataset:\n",
      "    _target_: mattergen.common.data.dataset.CrystalDataset.from_cache_path\n",
      "    cache_path: datasets/cache/alex_mp_20/val\n",
      "    dataset_transforms:\n",
      "    - _partial_: true\n",
      "      _target_: mattergen.common.data.dataset_transform.filter_sparse_properties\n",
      "    properties:\n",
      "    - space_group\n",
      "    transforms:\n",
      "    - _partial_: true\n",
      "      _target_: mattergen.common.data.transform.symmetrize_lattice\n",
      "    - _partial_: true\n",
      "      _target_: mattergen.common.data.transform.set_chemical_system_string\n",
      "lightning_module:\n",
      "  _target_: mattergen.diffusion.lightning_module.DiffusionLightningModule\n",
      "  diffusion_module:\n",
      "    _target_: mattergen.diffusion.diffusion_module.DiffusionModule\n",
      "    corruption:\n",
      "      _target_: mattergen.diffusion.corruption.multi_corruption.MultiCorruption\n",
      "      discrete_corruptions:\n",
      "        atomic_numbers:\n",
      "          _target_: mattergen.diffusion.corruption.d3pm_corruption.D3PMCorruption\n",
      "          d3pm:\n",
      "            _target_: mattergen.diffusion.d3pm.d3pm.MaskDiffusion\n",
      "            dim: 101\n",
      "            schedule:\n",
      "              _target_: mattergen.diffusion.d3pm.d3pm.create_discrete_diffusion_schedule\n",
      "              kind: standard\n",
      "              num_steps: 1000\n",
      "          offset: 1\n",
      "      sdes:\n",
      "        cell:\n",
      "          _target_: mattergen.common.diffusion.corruption.LatticeVPSDE.from_vpsde_config\n",
      "          vpsde_config:\n",
      "            beta_max: 20\n",
      "            beta_min: 0.1\n",
      "            limit_density: 0.05771451654022283\n",
      "            limit_var_scaling_constant: 0.25\n",
      "        pos:\n",
      "          _target_: mattergen.common.diffusion.corruption.NumAtomsVarianceAdjustedWrappedVESDE\n",
      "          limit_info_key: num_atoms\n",
      "          sigma_max: 5.0\n",
      "          wrapping_boundary: 1.0\n",
      "    loss_fn:\n",
      "      _target_: mattergen.common.loss.MaterialsLoss\n",
      "      d3pm_hybrid_lambda: 0.01\n",
      "      include_atomic_numbers: true\n",
      "      include_cell: true\n",
      "      include_pos: true\n",
      "      reduce: sum\n",
      "      weights:\n",
      "        atomic_numbers: 1.0\n",
      "        cell: 1.0\n",
      "        pos: 0.1\n",
      "    model:\n",
      "      _target_: mattergen.adapter.GemNetTAdapter\n",
      "      atom_type_diffusion: mask\n",
      "      denoise_atom_types: true\n",
      "      gemnet:\n",
      "        _target_: mattergen.common.gemnet.gemnet_ctrl.GemNetTCtrl\n",
      "        atom_embedding:\n",
      "          _target_: mattergen.common.gemnet.layers.embedding_block.AtomEmbedding\n",
      "          emb_size: 512\n",
      "          with_mask_type: true\n",
      "        condition_on_adapt:\n",
      "        - space_group\n",
      "        cutoff: 7.0\n",
      "        emb_size_atom: 512\n",
      "        emb_size_edge: 512\n",
      "        latent_dim: 512\n",
      "        max_cell_images_per_dim: 5\n",
      "        max_neighbors: 50\n",
      "        num_blocks: 4\n",
      "        num_targets: 1\n",
      "        otf_graph: true\n",
      "        regress_stress: true\n",
      "        scale_file: /scratch/amlt_code/mattergen/common/gemnet/gemnet-dT.json\n",
      "      hidden_dim: 512\n",
      "      property_embeddings: {}\n",
      "      property_embeddings_adapt:\n",
      "        space_group:\n",
      "          _target_: mattergen.property_embeddings.PropertyEmbedding\n",
      "          conditional_embedding_module:\n",
      "            _target_: mattergen.property_embeddings.SpaceGroupEmbeddingVector\n",
      "            hidden_dim: 512\n",
      "          name: space_group\n",
      "          scaler:\n",
      "            _target_: torch.nn.Identity\n",
      "          unconditional_embedding_module:\n",
      "            _target_: mattergen.property_embeddings.EmbeddingVector\n",
      "            hidden_dim: 512\n",
      "    pre_corruption_fn:\n",
      "      _target_: mattergen.property_embeddings.SetEmbeddingType\n",
      "      dropout_fields_iid: false\n",
      "      p_unconditional: 0.2\n",
      "  optimizer_partial:\n",
      "    _partial_: true\n",
      "    _target_: torch.optim.Adam\n",
      "    lr: 5.0e-06\n",
      "  scheduler_partials:\n",
      "  - frequency: 1\n",
      "    interval: epoch\n",
      "    monitor: loss_train\n",
      "    scheduler:\n",
      "      _partial_: true\n",
      "      _target_: torch.optim.lr_scheduler.ReduceLROnPlateau\n",
      "      factor: 0.6\n",
      "      min_lr: 1.0e-06\n",
      "      patience: 100\n",
      "      verbose: true\n",
      "    strict: true\n",
      "trainer:\n",
      "  _target_: pytorch_lightning.Trainer\n",
      "  accelerator: gpu\n",
      "  accumulate_grad_batches: 1\n",
      "  callbacks:\n",
      "  - _target_: pytorch_lightning.callbacks.LearningRateMonitor\n",
      "    log_momentum: false\n",
      "    logging_interval: step\n",
      "  - _target_: pytorch_lightning.callbacks.ModelCheckpoint\n",
      "    every_n_epochs: 1\n",
      "    filename: '{epoch}-{loss_val:.2f}'\n",
      "    mode: min\n",
      "    monitor: loss_val\n",
      "    save_last: true\n",
      "    save_top_k: 1\n",
      "    verbose: false\n",
      "  - _target_: pytorch_lightning.callbacks.TQDMProgressBar\n",
      "    refresh_rate: 50\n",
      "  - _target_: mattergen.common.data.callback.SetPropertyScalers\n",
      "  check_val_every_n_epoch: 1\n",
      "  devices: 8\n",
      "  gradient_clip_algorithm: value\n",
      "  gradient_clip_val: 0.5\n",
      "  logger:\n",
      "    _target_: pytorch_lightning.loggers.WandbLogger\n",
      "    job_type: train_finetune\n",
      "    project: crystal-generation\n",
      "    settings:\n",
      "      _save_requirements: false\n",
      "      _target_: wandb.Settings\n",
      "      start_method: fork\n",
      "  max_epochs: 200\n",
      "  num_nodes: 1\n",
      "  precision: 32\n",
      "  strategy:\n",
      "    _target_: pytorch_lightning.strategies.ddp.DDPStrategy\n",
      "    find_unused_parameters: true\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubt/Downloads/mattergen/mattergen/generator.py:324: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  with hydra.initialize_config_dir(os.path.abspath(str(sampling_config_path))):\n",
      "INFO:mattergen.common.utils.eval_utils:Loading model from checkpoint: /home/ubt/.cache/huggingface/hub/models--microsoft--mattergen/snapshots/2092423e1f1ab5f7d792142257f0477a57628105/checkpoints/space_group/checkpoints/last.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sampling config:\n",
      "sampler_partial:\n",
      "  _target_: mattergen.diffusion.sampling.classifier_free_guidance.GuidedPredictorCorrector.from_pl_module\n",
      "  'N': 1000\n",
      "  eps_t: 0.001\n",
      "  _partial_: true\n",
      "  guidance_scale: 5.0\n",
      "  remove_conditioning_fn:\n",
      "    _target_: mattergen.property_embeddings.SetUnconditionalEmbeddingType\n",
      "  keep_conditioning_fn:\n",
      "    _target_: mattergen.property_embeddings.SetConditionalEmbeddingType\n",
      "  predictor_partials:\n",
      "    pos:\n",
      "      _target_: mattergen.diffusion.wrapped.wrapped_predictors_correctors.WrappedAncestralSamplingPredictor\n",
      "      _partial_: true\n",
      "    cell:\n",
      "      _target_: mattergen.common.diffusion.predictors_correctors.LatticeAncestralSamplingPredictor\n",
      "      _partial_: true\n",
      "    atomic_numbers:\n",
      "      _target_: mattergen.diffusion.d3pm.d3pm_predictors_correctors.D3PMAncestralSamplingPredictor\n",
      "      predict_x0: true\n",
      "      _partial_: true\n",
      "  corrector_partials:\n",
      "    pos:\n",
      "      _target_: mattergen.diffusion.wrapped.wrapped_predictors_correctors.WrappedLangevinCorrector\n",
      "      _partial_: true\n",
      "      max_step_size: 1000000.0\n",
      "      snr: 0.4\n",
      "    cell:\n",
      "      _target_: mattergen.common.diffusion.predictors_correctors.LatticeLangevinDiffCorrector\n",
      "      _partial_: true\n",
      "      max_step_size: 1000000.0\n",
      "      snr: 0.2\n",
      "  n_steps_corrector: 1\n",
      "condition_loader_partial:\n",
      "  _partial_: true\n",
      "  _target_: mattergen.common.data.condition_factory.get_number_of_atoms_condition_loader\n",
      "  num_atoms_distribution: ALEX_MP_20\n",
      "  batch_size: 16\n",
      "  num_samples: 16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20936f1f006c4867b5470ce32b8736de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   0%|          | 0/1 [04:22<?, ?it/s]\n",
      "/home/ubt/Downloads/mattergen/mattergen/diffusion/sampling/classifier_free_guidance.py:50: SyntaxWarning: invalid escape sequence '\\g'\n",
      "  \"\"\"For each field, regardless of whether the corruption process is SDE or D3PM, we guide the score in the same way here,\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     13\u001b[39m checkpoint_info = MatterGenCheckpointInfo.from_hf_hub(\n\u001b[32m     14\u001b[39m     pretrained_name, config_overrides=[]\n\u001b[32m     15\u001b[39m )\n\u001b[32m     16\u001b[39m generator = CrystalGenerator(\n\u001b[32m     17\u001b[39m     checkpoint_info=checkpoint_info,\n\u001b[32m     18\u001b[39m     properties_to_condition_on=properties_to_condition_on,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     target_compositions_dict=target_compositions,\n\u001b[32m     27\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m generated_structures = \u001b[43mgenerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/mattergen/mattergen/generator.py:370\u001b[39m, in \u001b[36mCrystalGenerator.generate\u001b[39m\u001b[34m(self, batch_size, num_batches, target_compositions_dict, output_dir)\u001b[39m\n\u001b[32m    367\u001b[39m sampler_partial = instantiate(sampling_config.sampler_partial)\n\u001b[32m    368\u001b[39m sampler = sampler_partial(pl_module=\u001b[38;5;28mself\u001b[39m.model)\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m generated_structures = \u001b[43mdraw_samples_from_sampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43msampler\u001b[49m\u001b[43m=\u001b[49m\u001b[43msampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcondition_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcondition_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproperties_to_condition_on\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mproperties_to_condition_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrecord_trajectories\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrecord_trajectories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m generated_structures\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/mattergen/mattergen/generator.py:58\u001b[39m, in \u001b[36mdraw_samples_from_sampler\u001b[39m\u001b[34m(sampler, condition_loader, properties_to_condition_on, output_path, cfg, record_trajectories)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m conditioning_data, mask \u001b[38;5;129;01min\u001b[39;00m tqdm(condition_loader, desc=\u001b[33m\"\u001b[39m\u001b[33mGenerating samples\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     55\u001b[39m \n\u001b[32m     56\u001b[39m     \u001b[38;5;66;03m# generate samples\u001b[39;00m\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m record_trajectories:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m         sample, mean, intermediate_samples = \u001b[43msampler\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample_with_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconditioning_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m         all_trajs_list.extend(list_of_time_steps_to_list_of_trajectories(intermediate_samples))\n\u001b[32m     60\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pyg/lib/python3.13/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/mattergen/mattergen/diffusion/sampling/pc_sampler.py:130\u001b[39m, in \u001b[36mPredictorCorrector.sample_with_record\u001b[39m\u001b[34m(self, conditioning_data, mask)\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;129m@torch\u001b[39m.no_grad()\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msample_with_record\u001b[39m(\n\u001b[32m    118\u001b[39m     \u001b[38;5;28mself\u001b[39m, conditioning_data: BatchedData, mask: Mapping[\u001b[38;5;28mstr\u001b[39m, torch.Tensor] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    119\u001b[39m ) -> SampleAndMeanAndRecords:\n\u001b[32m    120\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create one sample for each of a batch of conditions.\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    122\u001b[39m \u001b[33;03m        conditioning_data: batched conditioning data. Even if you think you don't want conditioning, you still need to pass a batch of conditions\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    128\u001b[39m \n\u001b[32m    129\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample_maybe_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconditioning_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pyg/lib/python3.13/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/mattergen/mattergen/diffusion/sampling/pc_sampler.py:157\u001b[39m, in \u001b[36mPredictorCorrector._sample_maybe_record\u001b[39m\u001b[34m(self, conditioning_data, mask, record)\u001b[39m\n\u001b[32m    155\u001b[39m mask = {k: v.to(\u001b[38;5;28mself\u001b[39m._device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m mask.items()}\n\u001b[32m    156\u001b[39m batch = _sample_prior(\u001b[38;5;28mself\u001b[39m._multi_corruption, conditioning_data, mask=mask)\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_denoise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pyg/lib/python3.13/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/mattergen/mattergen/diffusion/sampling/pc_sampler.py:205\u001b[39m, in \u001b[36mPredictorCorrector._denoise\u001b[39m\u001b[34m(self, batch, mask, record)\u001b[39m\n\u001b[32m    200\u001b[39m         batch, mean_batch = _mask_replace(\n\u001b[32m    201\u001b[39m             samples_means=samples_means, batch=batch, mean_batch=mean_batch, mask=mask\n\u001b[32m    202\u001b[39m         )\n\u001b[32m    204\u001b[39m \u001b[38;5;66;03m# Predictor updates\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m score = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_score_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m predictor_fns = {\n\u001b[32m    207\u001b[39m     k: predictor.update_given_score \u001b[38;5;28;01mfor\u001b[39;00m k, predictor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._predictors.items()\n\u001b[32m    208\u001b[39m }\n\u001b[32m    209\u001b[39m samples_means = apply(\n\u001b[32m    210\u001b[39m     fns=predictor_fns,\n\u001b[32m    211\u001b[39m     x=batch,\n\u001b[32m   (...)\u001b[39m\u001b[32m    214\u001b[39m     batch_idx=\u001b[38;5;28mself\u001b[39m._multi_corruption._get_batch_indices(batch),\n\u001b[32m    215\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/mattergen/mattergen/diffusion/sampling/classifier_free_guidance.py:75\u001b[39m, in \u001b[36mGuidedPredictorCorrector._score_fn\u001b[39m\u001b[34m(self, x, t)\u001b[39m\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m get_unconditional_score()\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     73\u001b[39m     \u001b[38;5;66;03m# guided_score = guidance_factor * conditional_score + (1-guidance_factor) * unconditional_score\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     conditional_score = \u001b[43mget_conditional_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     unconditional_score = get_unconditional_score()\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m unconditional_score.replace(\n\u001b[32m     78\u001b[39m         **{\n\u001b[32m     79\u001b[39m             k: torch.lerp(\n\u001b[32m   (...)\u001b[39m\u001b[32m     83\u001b[39m         }\n\u001b[32m     84\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/mattergen/mattergen/diffusion/sampling/classifier_free_guidance.py:64\u001b[39m, in \u001b[36mGuidedPredictorCorrector._score_fn.<locals>.get_conditional_score\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_conditional_score\u001b[39m():\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mGuidedPredictorCorrector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_score_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_keep_conditioning_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mt\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/mattergen/mattergen/diffusion/sampling/pc_sampler.py:94\u001b[39m, in \u001b[36mPredictorCorrector._score_fn\u001b[39m\u001b[34m(self, x, t)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_score_fn\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Diffusable, t: torch.Tensor) -> Diffusable:\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_diffusion_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscore_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/mattergen/mattergen/diffusion/diffusion_module.py:129\u001b[39m, in \u001b[36mDiffusionModule.score_fn\u001b[39m\u001b[34m(self, x, t)\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mscore_fn\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: T, t: torch.Tensor) -> T:\n\u001b[32m    120\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Calculate the score of a batch of data at a given timestep\u001b[39;00m\n\u001b[32m    121\u001b[39m \n\u001b[32m    122\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    127\u001b[39m \u001b[33;03m        score: score of the batch of data at the given timestep\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     model_out: T = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m     fns = {k: convert_model_out_to_score \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.corruption.sdes.keys()}\n\u001b[32m    132\u001b[39m     scores = apply(\n\u001b[32m    133\u001b[39m         fns=fns,\n\u001b[32m    134\u001b[39m         model_out=model_out,\n\u001b[32m   (...)\u001b[39m\u001b[32m    138\u001b[39m         batch_idx=\u001b[38;5;28mself\u001b[39m.corruption._get_batch_indices(x),\n\u001b[32m    139\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pyg/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pyg/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/mattergen/mattergen/adapter.py:91\u001b[39m, in \u001b[36mGemNetTAdapter.forward\u001b[39m\u001b[34m(self, x, t)\u001b[39m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m     85\u001b[39m         \u001b[38;5;66;03m# no values have been provided for the conditional field,\u001b[39;00m\n\u001b[32m     86\u001b[39m         \u001b[38;5;66;03m# interpret this as the user wanting an unconditional score\u001b[39;00m\n\u001b[32m     87\u001b[39m         conditions_adapt_mask_dict[cond_field] = torch.ones_like(\n\u001b[32m     88\u001b[39m             x[\u001b[33m\"\u001b[39m\u001b[33mnum_atoms\u001b[39m\u001b[33m\"\u001b[39m], dtype=torch.bool\n\u001b[32m     89\u001b[39m         ).reshape(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgemnet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m    \u001b[49m\u001b[43mz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mz_per_crystal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrac_coords\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrac_coords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43matom_types\u001b[49m\u001b[43m=\u001b[49m\u001b[43matom_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_atoms\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_atoms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43mangles\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlattice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlattice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# we construct the graph on the fly, hence pass None for these:\u001b[39;49;00m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mto_jimages\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_bonds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcond_adapt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconditions_adapt_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcond_adapt_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconditions_adapt_mask_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# when True use unconditional embedding\u001b[39;49;00m\n\u001b[32m    106\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m pred_atom_types = \u001b[38;5;28mself\u001b[39m.fc_atom(output.node_embeddings)\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m get_chemgraph_from_denoiser_output(\n\u001b[32m    111\u001b[39m     pred_atom_types=pred_atom_types,\n\u001b[32m    112\u001b[39m     pred_lattice_eps=output.stress,\n\u001b[32m   (...)\u001b[39m\u001b[32m    116\u001b[39m     x_input=x,\n\u001b[32m    117\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pyg/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pyg/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/mattergen/mattergen/common/gemnet/gemnet_ctrl.py:227\u001b[39m, in \u001b[36mGemNetTCtrl.forward\u001b[39m\u001b[34m(self, z, frac_coords, atom_types, num_atoms, batch, lengths, angles, edge_index, to_jimages, num_bonds, lattice, charges, cond_adapt, cond_adapt_mask)\u001b[39m\n\u001b[32m    225\u001b[39m     E_t += E\n\u001b[32m    226\u001b[39m     rbf_lattice = \u001b[38;5;28mself\u001b[39m.mlp_rbf_lattice(rbf)\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     lattice_update += \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlattice_out_blocks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m        \u001b[49m\u001b[43medge_emb\u001b[49m\u001b[43m=\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdistance_vec\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdistance_vec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlattice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdistorted_lattice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrbf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrbf_lattice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnormalize_score\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    237\u001b[39m nMolecules = torch.max(batch) + \u001b[32m1\u001b[39m\n\u001b[32m    239\u001b[39m \u001b[38;5;66;03m# always use sum aggregation\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pyg/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pyg/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/mattergen/mattergen/common/gemnet/gemnet.py:110\u001b[39m, in \u001b[36mRBFBasedLatticeUpdateBlockFrac.forward\u001b[39m\u001b[34m(self, edge_emb, edge_index, distance_vec, lattice, batch, rbf, normalize_score)\u001b[39m\n\u001b[32m    108\u001b[39m edge_scores = \u001b[38;5;28mself\u001b[39m.compute_score_per_edge(edge_emb=edge_emb, rbf=rbf)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m normalize_score:\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     num_edges = scatter(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistance_vec\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, batch[edge_index[\u001b[32m0\u001b[39m]])\n\u001b[32m    111\u001b[39m     edge_scores /= num_edges[batch[edge_index[\u001b[32m0\u001b[39m]], \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    112\u001b[39m outs = []\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "output_path = \"results\"\n",
    "pretrained_name = \"mattergen_base\"\n",
    "batch_size = 16\n",
    "num_batches = 1\n",
    "properties_to_condition_on = {}\n",
    "sampling_config_path = None\n",
    "sampling_config_name = \"default\"\n",
    "sampling_config_overrides = []\n",
    "record_trajectories = True\n",
    "diffusion_guidance_factor = 5.0\n",
    "target_compositions = []\n",
    "\n",
    "checkpoint_info = MatterGenCheckpointInfo.from_hf_hub(\n",
    "    pretrained_name, config_overrides=[]\n",
    ")\n",
    "generator = CrystalGenerator(\n",
    "    checkpoint_info=checkpoint_info,\n",
    "    properties_to_condition_on=properties_to_condition_on,\n",
    "    batch_size=batch_size,\n",
    "    num_batches=num_batches,\n",
    "    sampling_config_name=sampling_config_name,\n",
    "    sampling_config_path=sampling_config_path,\n",
    "    sampling_config_overrides=sampling_config_overrides,\n",
    "    record_trajectories=record_trajectories,\n",
    "    diffusion_guidance_factor=diffusion_guidance_factor,\n",
    "    target_compositions_dict=target_compositions,\n",
    ")\n",
    "generated_structures = generator.generate(output_dir=Path(output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b74dec9",
   "metadata": {},
   "source": [
    "## Preprocessea custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9681989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test/raw1_processed/val.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e77bc02dd5674837b7c9bccbfc80c5c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing CIFs:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5611f8c66f5e48459653a3df8ebf4b15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting structures to numpy:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing cached dataset in datasets/cache/raw1/val.\n",
      "Processing test/raw1_processed/test.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5de6d42eb1c42e2bdf82048b8964675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing CIFs:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5976c0d14b254abbbba4e7c8563121fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting structures to numpy:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing cached dataset in datasets/cache/raw1/test.\n",
      "Processing test/raw1_processed/train.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ac3ab1ecff4abe81e112b8c5d07e25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing CIFs:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c91551dcbed433eaace67556d0b9ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting structures to numpy:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing cached dataset in datasets/cache/raw1/train.\n"
     ]
    }
   ],
   "source": [
    "from mattergen.scripts import csv_to_dataset\n",
    "import sys\n",
    "sys.argv = [\n",
    "    \"notebook\",  # dummy script name\n",
    "    \"--csv-folder\", \"test/raw1_processed\",\n",
    "    \"--dataset-name\", \"raw1\",\n",
    "    \"--cache-folder\", \"datasets/cache\"\n",
    "]\n",
    "\n",
    "csv_to_dataset.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3cfb0d",
   "metadata": {},
   "source": [
    "## Training from a custom dataset\n",
    "\n",
    "### Create datamodule/raw1.yaml file (batch size, epoch limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3776faa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "provider=hydra, path=pkg://hydra.conf\n",
      "provider=main, path=file:///home/ubt/Downloads/mattergen/mattergen/conf\n",
      "provider=schema, path=structured://\n",
      "provider=hydra, path=pkg://hydra.conf\n",
      "provider=main, path=file:///home/ubt/Downloads/mattergen/mattergen/conf\n",
      "provider=schema, path=structured://\n",
      "data_module:\n",
      "  _target_: mattergen.common.data.datamodule.CrystDataModule\n",
      "  _recursive_: true\n",
      "  properties: []\n",
      "  dataset_transforms:\n",
      "  - _target_: mattergen.common.data.dataset_transform.filter_sparse_properties\n",
      "    _partial_: true\n",
      "  transforms:\n",
      "  - _target_: mattergen.common.data.transform.symmetrize_lattice\n",
      "    _partial_: true\n",
      "  - _target_: mattergen.common.data.transform.set_chemical_system_string\n",
      "    _partial_: true\n",
      "  average_density: 0.07\n",
      "  root_dir: /home/ubt/Downloads/mattergen/mattergen/../datasets/cache/raw1\n",
      "  train_dataset:\n",
      "    _target_: mattergen.common.data.dataset.CrystalDataset.from_cache_path\n",
      "    cache_path: /home/ubt/Downloads/mattergen/mattergen/../datasets/cache/raw1/train\n",
      "    properties: []\n",
      "    transforms:\n",
      "    - _target_: mattergen.common.data.transform.symmetrize_lattice\n",
      "      _partial_: true\n",
      "    - _target_: mattergen.common.data.transform.set_chemical_system_string\n",
      "      _partial_: true\n",
      "    dataset_transforms:\n",
      "    - _target_: mattergen.common.data.dataset_transform.filter_sparse_properties\n",
      "      _partial_: true\n",
      "  val_dataset:\n",
      "    _target_: mattergen.common.data.dataset.CrystalDataset.from_cache_path\n",
      "    cache_path: /home/ubt/Downloads/mattergen/mattergen/../datasets/cache/raw1/val\n",
      "    properties: []\n",
      "    transforms:\n",
      "    - _target_: mattergen.common.data.transform.symmetrize_lattice\n",
      "      _partial_: true\n",
      "    - _target_: mattergen.common.data.transform.set_chemical_system_string\n",
      "      _partial_: true\n",
      "    dataset_transforms:\n",
      "    - _target_: mattergen.common.data.dataset_transform.filter_sparse_properties\n",
      "      _partial_: true\n",
      "  num_workers:\n",
      "    train: 0\n",
      "    val: 0\n",
      "  batch_size:\n",
      "    train: 1\n",
      "    val: 1\n",
      "  max_epochs: 100\n",
      "trainer:\n",
      "  _target_: pytorch_lightning.Trainer\n",
      "  accelerator: gpu\n",
      "  devices: 1\n",
      "  num_nodes: 1\n",
      "  precision: 32\n",
      "  max_epochs: 100\n",
      "  accumulate_grad_batches: 1\n",
      "  gradient_clip_val: 0.5\n",
      "  gradient_clip_algorithm: value\n",
      "  check_val_every_n_epoch: 5\n",
      "  strategy: auto\n",
      "  callbacks:\n",
      "  - _target_: pytorch_lightning.callbacks.LearningRateMonitor\n",
      "    logging_interval: step\n",
      "    log_momentum: false\n",
      "  - _target_: pytorch_lightning.callbacks.ModelCheckpoint\n",
      "    monitor: loss_val\n",
      "    mode: min\n",
      "    save_top_k: 1\n",
      "    save_last: true\n",
      "    verbose: false\n",
      "    every_n_epochs: 1\n",
      "    filename: '{epoch}-{loss_val:.2f}'\n",
      "  - _target_: pytorch_lightning.callbacks.TQDMProgressBar\n",
      "    refresh_rate: 50\n",
      "  - _target_: mattergen.common.data.callback.SetPropertyScalers\n",
      "lightning_module:\n",
      "  _target_: mattergen.diffusion.lightning_module.DiffusionLightningModule\n",
      "  optimizer_partial:\n",
      "    lr: 0.0001\n",
      "    _target_: torch.optim.Adam\n",
      "    _partial_: true\n",
      "  scheduler_partials:\n",
      "  - scheduler:\n",
      "      _target_: torch.optim.lr_scheduler.ReduceLROnPlateau\n",
      "      factor: 0.6\n",
      "      patience: 100\n",
      "      min_lr: 1.0e-06\n",
      "      verbose: true\n",
      "      _partial_: true\n",
      "    interval: epoch\n",
      "    frequency: 1\n",
      "    monitor: loss_train\n",
      "    strict: true\n",
      "  diffusion_module:\n",
      "    _target_: mattergen.diffusion.diffusion_module.DiffusionModule\n",
      "    loss_fn:\n",
      "      _target_: mattergen.common.loss.MaterialsLoss\n",
      "      reduce: sum\n",
      "      include_pos: true\n",
      "      include_cell: true\n",
      "      include_atomic_numbers: true\n",
      "      d3pm_hybrid_lambda: 0.01\n",
      "      weights:\n",
      "        cell: 1.0\n",
      "        pos: 0.1\n",
      "        atomic_numbers: 1.0\n",
      "    model:\n",
      "      _target_: mattergen.denoiser.GemNetTDenoiser\n",
      "      hidden_dim: 512\n",
      "      gemnet:\n",
      "        _target_: mattergen.common.gemnet.gemnet.GemNetT\n",
      "        num_targets: 1\n",
      "        latent_dim: 512\n",
      "        atom_embedding:\n",
      "          _target_: mattergen.common.gemnet.layers.embedding_block.AtomEmbedding\n",
      "          emb_size: 512\n",
      "          with_mask_type: true\n",
      "        emb_size_atom: 512\n",
      "        emb_size_edge: 512\n",
      "        max_neighbors: 50\n",
      "        max_cell_images_per_dim: 5\n",
      "        cutoff: 7.0\n",
      "        num_blocks: 4\n",
      "        regress_stress: true\n",
      "        otf_graph: true\n",
      "        scale_file: /home/ubt/Downloads/mattergen/mattergen/common/gemnet/gemnet-dT.json\n",
      "      denoise_atom_types: true\n",
      "      atom_type_diffusion: mask\n",
      "      property_embeddings_adapt: {}\n",
      "      property_embeddings: {}\n",
      "    corruption:\n",
      "      _target_: mattergen.diffusion.corruption.multi_corruption.MultiCorruption\n",
      "      sdes:\n",
      "        pos:\n",
      "          _target_: mattergen.common.diffusion.corruption.NumAtomsVarianceAdjustedWrappedVESDE\n",
      "          wrapping_boundary: 1.0\n",
      "          sigma_max: 5.0\n",
      "          limit_info_key: num_atoms\n",
      "        cell:\n",
      "          _target_: mattergen.common.diffusion.corruption.LatticeVPSDE.from_vpsde_config\n",
      "          vpsde_config:\n",
      "            beta_min: 0.1\n",
      "            beta_max: 20\n",
      "            limit_density: 0.07\n",
      "            limit_var_scaling_constant: 0.25\n",
      "      discrete_corruptions:\n",
      "        atomic_numbers:\n",
      "          _target_: mattergen.diffusion.corruption.d3pm_corruption.D3PMCorruption\n",
      "          offset: 1\n",
      "          d3pm:\n",
      "            _target_: mattergen.diffusion.d3pm.d3pm.MaskDiffusion\n",
      "            dim: 101\n",
      "            schedule:\n",
      "              _target_: mattergen.diffusion.d3pm.d3pm.create_discrete_diffusion_schedule\n",
      "              kind: standard\n",
      "              num_steps: 1000\n",
      "    pre_corruption_fn:\n",
      "      _target_: mattergen.property_embeddings.SetEmbeddingType\n",
      "      p_unconditional: 0.2\n",
      "      dropout_fields_iid: false\n",
      "auto_resume: true\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ubt/miniconda3/envs/pyg/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/ubt/miniconda3/envs/pyg/lib/python3.13/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name             | Type            | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | diffusion_module | DiffusionModule | 44.6 M | train\n",
      "-------------------------------------------------------------\n",
      "44.6 M    Trainable params\n",
      "22        Non-trainable params\n",
      "44.6 M    Total params\n",
      "178.248   Total estimated model params size (MB)\n",
      "1006      Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d4f48dbf1d41969aac257e4ebb8eee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubt/miniconda3/envs/pyg/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/ubt/miniconda3/envs/pyg/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e617066ef4a4cabb71708e84a22c083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a16e44566c594085bf898d3a917ed4ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "223754a384d14432b56a7695384e28d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19f6c8daf9a24d529b81c9a6165b4050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7278146401ad48f58b293a2ceb6eea91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3005292a06444fa398f72931aef43782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47c81d8b45854beb89e73472e3d41f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c745d80fa34a1aabf6ce5c7961a9f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9efea17ae2444b9282f40e5b2825a0d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "511f29fe63354e71987c0e9e380c7a19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d30525a0cc043f8ac5bd8eff58a9343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23bff003c6ae424e85d2a97f147bcc06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c8ff91c3b24b01a44ff0b58ebd68df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64d03e5fb8b84b15a9e0b5abf28cc2fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eb412da677f48fdb33717ee8515af3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a874b3c170fb480b8632e87bb27712fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8f0d6d2c1e64691b19b596dce6304ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc37b20461f741ebacfebcfc5730f2f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f77b965a48474abb215c554b10c17e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c090f4c75534865a90e6e1bbd7e5d39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e76a331ba1df4de893c808e65611462f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    }
   ],
   "source": [
    "from mattergen.scripts.run import mattergen_main\n",
    "import sys\n",
    "\n",
    "sys.argv = [\n",
    "    \"notebook\",\n",
    "    \"data_module=raw1\",\n",
    "    \"~trainer.logger\",\n",
    "    \"trainer.strategy=auto\",  # ✅ fix here\n",
    "    \"trainer.num_nodes=1\",            # optional\n",
    "    \"trainer.accelerator=gpu\"         # optional but recommended\n",
    "]\n",
    "\n",
    "mattergen_main()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb71a27a",
   "metadata": {},
   "source": [
    "## Load custom training data and execute generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8322b40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Literal, Optional\n",
    "import numpy as np\n",
    "from hydra import compose, initialize_config_dir\n",
    "from omegaconf import DictConfig\n",
    "import os\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MatterGenCheckpointInfo:\n",
    "    model_path: Path\n",
    "    config_path: Path\n",
    "    load_epoch: Optional[int | Literal[\"best\", \"last\"]] = \"last\"\n",
    "    config_overrides: list[str] = field(default_factory=list)\n",
    "    strict_checkpoint_loading: bool = True  # <-- reintroduced for compatibility\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.model_path = Path(self.model_path).resolve()\n",
    "        self.config_path = Path(self.config_path).resolve()\n",
    "\n",
    "        if not self.model_path.is_dir():\n",
    "            raise FileNotFoundError(f\"Model directory does not exist: {self.model_path}\")\n",
    "        if not (self.config_path / \"config.yaml\").exists():\n",
    "            raise FileNotFoundError(f\"'config.yaml' not found in {self.config_path}\")\n",
    "\n",
    "    @property\n",
    "    def config(self) -> DictConfig:\n",
    "        from hydra.core.global_hydra import GlobalHydra\n",
    "        if GlobalHydra.instance().is_initialized():\n",
    "            GlobalHydra.instance().clear()\n",
    "\n",
    "        with initialize_config_dir(config_dir=str(self.config_path), version_base=\"1.1\"):\n",
    "            return compose(config_name=\"config\", overrides=self.config_overrides)\n",
    "\n",
    "    @property\n",
    "    def checkpoint_path(self) -> Path:\n",
    "        ckpts = sorted(self.model_path.glob(\"*.ckpt\"))\n",
    "        if not ckpts:\n",
    "            raise FileNotFoundError(f\"No checkpoints found in {self.model_path}\")\n",
    "\n",
    "        if self.load_epoch == \"last\":\n",
    "            last_ckpt = [ckpt for ckpt in ckpts if \"last.ckpt\" in ckpt.name]\n",
    "            if not last_ckpt:\n",
    "                raise ValueError(\"No 'last.ckpt' file found.\")\n",
    "            return last_ckpt[0]\n",
    "\n",
    "        if self.load_epoch == \"best\":\n",
    "            ckpts_named = [ckpt for ckpt in ckpts if \"val_loss\" in ckpt.name]\n",
    "            if not ckpts_named:\n",
    "                raise ValueError(\"No checkpoint with validation loss in filename.\")\n",
    "            ckpts_named.sort(key=lambda x: float(x.name.split(\"val_loss=\")[-1].split(\".\")[0]))\n",
    "            return ckpts_named[0]\n",
    "\n",
    "        if isinstance(self.load_epoch, int):\n",
    "            for ckpt in ckpts:\n",
    "                if f\"epoch={self.load_epoch}\" in ckpt.name:\n",
    "                    return ckpt\n",
    "            raise ValueError(f\"Checkpoint for epoch={self.load_epoch} not found.\")\n",
    "\n",
    "        raise ValueError(f\"Invalid load_epoch: {self.load_epoch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "476c995f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model config:\n",
      "provider=hydra, path=pkg://hydra.conf\n",
      "provider=main, path=file:///home/ubt/Downloads/mattergen/outputs/singlerun/2025-05-07/05-10-07/lightning_logs/version_0\n",
      "provider=schema, path=structured://\n",
      "auto_resume: true\n",
      "checkpoint_path: null\n",
      "data_module:\n",
      "  _recursive_: true\n",
      "  _target_: mattergen.common.data.datamodule.CrystDataModule\n",
      "  average_density: 0.07\n",
      "  batch_size:\n",
      "    train: 1\n",
      "    val: 1\n",
      "  dataset_transforms:\n",
      "  - _partial_: true\n",
      "    _target_: mattergen.common.data.dataset_transform.filter_sparse_properties\n",
      "  max_epochs: 100\n",
      "  num_workers:\n",
      "    train: 0\n",
      "    val: 0\n",
      "  properties: []\n",
      "  root_dir: /home/ubt/Downloads/mattergen/mattergen/../datasets/cache/raw1\n",
      "  train_dataset:\n",
      "    _target_: mattergen.common.data.dataset.CrystalDataset.from_cache_path\n",
      "    cache_path: /home/ubt/Downloads/mattergen/mattergen/../datasets/cache/raw1/train\n",
      "    dataset_transforms:\n",
      "    - _partial_: true\n",
      "      _target_: mattergen.common.data.dataset_transform.filter_sparse_properties\n",
      "    properties: []\n",
      "    transforms:\n",
      "    - _partial_: true\n",
      "      _target_: mattergen.common.data.transform.symmetrize_lattice\n",
      "    - _partial_: true\n",
      "      _target_: mattergen.common.data.transform.set_chemical_system_string\n",
      "  transforms:\n",
      "  - _partial_: true\n",
      "    _target_: mattergen.common.data.transform.symmetrize_lattice\n",
      "  - _partial_: true\n",
      "    _target_: mattergen.common.data.transform.set_chemical_system_string\n",
      "  val_dataset:\n",
      "    _target_: mattergen.common.data.dataset.CrystalDataset.from_cache_path\n",
      "    cache_path: /home/ubt/Downloads/mattergen/mattergen/../datasets/cache/raw1/val\n",
      "    dataset_transforms:\n",
      "    - _partial_: true\n",
      "      _target_: mattergen.common.data.dataset_transform.filter_sparse_properties\n",
      "    properties: []\n",
      "    transforms:\n",
      "    - _partial_: true\n",
      "      _target_: mattergen.common.data.transform.symmetrize_lattice\n",
      "    - _partial_: true\n",
      "      _target_: mattergen.common.data.transform.set_chemical_system_string\n",
      "lightning_module:\n",
      "  _target_: mattergen.diffusion.lightning_module.DiffusionLightningModule\n",
      "  diffusion_module:\n",
      "    _target_: mattergen.diffusion.diffusion_module.DiffusionModule\n",
      "    corruption:\n",
      "      _target_: mattergen.diffusion.corruption.multi_corruption.MultiCorruption\n",
      "      discrete_corruptions:\n",
      "        atomic_numbers:\n",
      "          _target_: mattergen.diffusion.corruption.d3pm_corruption.D3PMCorruption\n",
      "          d3pm:\n",
      "            _target_: mattergen.diffusion.d3pm.d3pm.MaskDiffusion\n",
      "            dim: 101\n",
      "            schedule:\n",
      "              _target_: mattergen.diffusion.d3pm.d3pm.create_discrete_diffusion_schedule\n",
      "              kind: standard\n",
      "              num_steps: 1000\n",
      "          offset: 1\n",
      "      sdes:\n",
      "        cell:\n",
      "          _target_: mattergen.common.diffusion.corruption.LatticeVPSDE.from_vpsde_config\n",
      "          vpsde_config:\n",
      "            beta_max: 20\n",
      "            beta_min: 0.1\n",
      "            limit_density: 0.07\n",
      "            limit_var_scaling_constant: 0.25\n",
      "        pos:\n",
      "          _target_: mattergen.common.diffusion.corruption.NumAtomsVarianceAdjustedWrappedVESDE\n",
      "          limit_info_key: num_atoms\n",
      "          sigma_max: 5.0\n",
      "          wrapping_boundary: 1.0\n",
      "    loss_fn:\n",
      "      _target_: mattergen.common.loss.MaterialsLoss\n",
      "      d3pm_hybrid_lambda: 0.01\n",
      "      include_atomic_numbers: true\n",
      "      include_cell: true\n",
      "      include_pos: true\n",
      "      reduce: sum\n",
      "      weights:\n",
      "        atomic_numbers: 1.0\n",
      "        cell: 1.0\n",
      "        pos: 0.1\n",
      "    model:\n",
      "      _target_: mattergen.denoiser.GemNetTDenoiser\n",
      "      atom_type_diffusion: mask\n",
      "      denoise_atom_types: true\n",
      "      gemnet:\n",
      "        _target_: mattergen.common.gemnet.gemnet.GemNetT\n",
      "        atom_embedding:\n",
      "          _target_: mattergen.common.gemnet.layers.embedding_block.AtomEmbedding\n",
      "          emb_size: 512\n",
      "          with_mask_type: true\n",
      "        cutoff: 7.0\n",
      "        emb_size_atom: 512\n",
      "        emb_size_edge: 512\n",
      "        latent_dim: 512\n",
      "        max_cell_images_per_dim: 5\n",
      "        max_neighbors: 50\n",
      "        num_blocks: 4\n",
      "        num_targets: 1\n",
      "        otf_graph: true\n",
      "        regress_stress: true\n",
      "        scale_file: /home/ubt/Downloads/mattergen/mattergen/common/gemnet/gemnet-dT.json\n",
      "      hidden_dim: 512\n",
      "      property_embeddings: {}\n",
      "      property_embeddings_adapt: {}\n",
      "    pre_corruption_fn:\n",
      "      _target_: mattergen.property_embeddings.SetEmbeddingType\n",
      "      dropout_fields_iid: false\n",
      "      p_unconditional: 0.2\n",
      "  optimizer_partial:\n",
      "    _partial_: true\n",
      "    _target_: torch.optim.Adam\n",
      "    lr: 0.0001\n",
      "  scheduler_partials:\n",
      "  - frequency: 1\n",
      "    interval: epoch\n",
      "    monitor: loss_train\n",
      "    scheduler:\n",
      "      _partial_: true\n",
      "      _target_: torch.optim.lr_scheduler.ReduceLROnPlateau\n",
      "      factor: 0.6\n",
      "      min_lr: 1.0e-06\n",
      "      patience: 100\n",
      "      verbose: true\n",
      "    strict: true\n",
      "load_original: false\n",
      "params: {}\n",
      "trainer:\n",
      "  _target_: pytorch_lightning.Trainer\n",
      "  accelerator: gpu\n",
      "  accumulate_grad_batches: 1\n",
      "  callbacks:\n",
      "  - _target_: pytorch_lightning.callbacks.LearningRateMonitor\n",
      "    log_momentum: false\n",
      "    logging_interval: step\n",
      "  - _target_: pytorch_lightning.callbacks.ModelCheckpoint\n",
      "    every_n_epochs: 1\n",
      "    filename: '{epoch}-{loss_val:.2f}'\n",
      "    mode: min\n",
      "    monitor: loss_val\n",
      "    save_last: true\n",
      "    save_top_k: 1\n",
      "    verbose: false\n",
      "  - _target_: pytorch_lightning.callbacks.TQDMProgressBar\n",
      "    refresh_rate: 50\n",
      "  - _target_: mattergen.common.data.callback.SetPropertyScalers\n",
      "  check_val_every_n_epoch: 5\n",
      "  devices: 1\n",
      "  gradient_clip_algorithm: value\n",
      "  gradient_clip_val: 0.5\n",
      "  max_epochs: 100\n",
      "  num_nodes: 1\n",
      "  precision: 32\n",
      "  strategy: auto\n",
      "\n",
      "provider=hydra, path=pkg://hydra.conf\n",
      "provider=main, path=file:///home/ubt/Downloads/mattergen/sampling_conf\n",
      "provider=schema, path=structured://\n",
      "\n",
      "Sampling config:\n",
      "sampler_partial:\n",
      "  _target_: mattergen.diffusion.sampling.classifier_free_guidance.GuidedPredictorCorrector.from_pl_module\n",
      "  'N': 1000\n",
      "  eps_t: 0.001\n",
      "  _partial_: true\n",
      "  guidance_scale: 0.0\n",
      "  remove_conditioning_fn:\n",
      "    _target_: mattergen.property_embeddings.SetUnconditionalEmbeddingType\n",
      "  keep_conditioning_fn:\n",
      "    _target_: mattergen.property_embeddings.SetConditionalEmbeddingType\n",
      "  predictor_partials:\n",
      "    pos:\n",
      "      _target_: mattergen.diffusion.wrapped.wrapped_predictors_correctors.WrappedAncestralSamplingPredictor\n",
      "      _partial_: true\n",
      "    cell:\n",
      "      _target_: mattergen.common.diffusion.predictors_correctors.LatticeAncestralSamplingPredictor\n",
      "      _partial_: true\n",
      "    atomic_numbers:\n",
      "      _target_: mattergen.diffusion.d3pm.d3pm_predictors_correctors.D3PMAncestralSamplingPredictor\n",
      "      predict_x0: true\n",
      "      _partial_: true\n",
      "  corrector_partials:\n",
      "    pos:\n",
      "      _target_: mattergen.diffusion.wrapped.wrapped_predictors_correctors.WrappedLangevinCorrector\n",
      "      _partial_: true\n",
      "      max_step_size: 1000000.0\n",
      "      snr: 0.4\n",
      "    cell:\n",
      "      _target_: mattergen.common.diffusion.predictors_correctors.LatticeLangevinDiffCorrector\n",
      "      _partial_: true\n",
      "      max_step_size: 1000000.0\n",
      "      snr: 0.2\n",
      "  n_steps_corrector: 1\n",
      "condition_loader_partial:\n",
      "  _partial_: true\n",
      "  _target_: mattergen.common.data.condition_factory.get_number_of_atoms_condition_loader\n",
      "  num_atoms_distribution: ALEX_MP_20\n",
      "  batch_size: 64\n",
      "  num_samples: 64\n",
      "\n",
      "[2025-05-07 05:32:19,035][mattergen.common.utils.eval_utils][INFO] - Loading model from checkpoint: /home/ubt/Downloads/mattergen/outputs/singlerun/2025-05-07/05-10-07/lightning_logs/version_0/checkpoints/last.ckpt\n",
      "provider=hydra, path=pkg://hydra.conf\n",
      "provider=main, path=file:///home/ubt/Downloads/mattergen/outputs/singlerun/2025-05-07/05-10-07/lightning_logs/version_0\n",
      "provider=schema, path=structured://\n",
      "provider=hydra, path=pkg://hydra.conf\n",
      "provider=main, path=file:///home/ubt/Downloads/mattergen/outputs/singlerun/2025-05-07/05-10-07/lightning_logs/version_0\n",
      "provider=schema, path=structured://\n",
      "provider=hydra, path=pkg://hydra.conf\n",
      "provider=main, path=file:///home/ubt/Downloads/mattergen/outputs/singlerun/2025-05-07/05-10-07/lightning_logs/version_0\n",
      "provider=schema, path=structured://\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f6929a52514ccabed3b38cb650e887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples: 100%|██████████| 1/1 [04:56<00:00, 296.02s/it]\n"
     ]
    }
   ],
   "source": [
    "# === Parameters ===\n",
    "cfg_path=\"./outputs/singlerun/2025-05-07/05-10-07/lightning_logs/version_0\"\n",
    "chk_path=f\"{cfg_path}/checkpoints\"\n",
    "output_path = \"results\"\n",
    "batch_size = 64\n",
    "num_batches = 1\n",
    "properties_to_condition_on = {}\n",
    "sampling_config_path = None\n",
    "sampling_config_name = \"default\"\n",
    "sampling_config_overrides = []\n",
    "record_trajectories = True\n",
    "diffusion_guidance_factor = 0.0\n",
    "target_compositions = []  # e.g., [{'Na': 4, 'Cl': 4, 'H2O': 4}]\n",
    "\n",
    "checkpoint_info = MatterGenCheckpointInfo(\n",
    "    model_path= Path(chk_path),\n",
    "    config_path=Path(cfg_path)\n",
    ")\n",
    "\n",
    "# ✅ Now create generator\n",
    "generator = CrystalGenerator(\n",
    "    checkpoint_info=checkpoint_info,\n",
    "    properties_to_condition_on=properties_to_condition_on,\n",
    "    batch_size=batch_size,\n",
    "    num_batches=num_batches,\n",
    "    sampling_config_name=sampling_config_name,\n",
    "    sampling_config_path=sampling_config_path,\n",
    "    sampling_config_overrides=sampling_config_overrides,\n",
    "    record_trajectories=record_trajectories,\n",
    "    diffusion_guidance_factor=diffusion_guidance_factor,\n",
    "    target_compositions_dict=target_compositions,\n",
    ")\n",
    "\n",
    "# ✅ Run generation\n",
    "generated_structures = generator.generate(output_dir=Path(output_path))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
